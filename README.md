# Introduction

This is a project involving the bottom-up, top-down, and left-corner CFG parsing schemas in Haskell to explore how humans and machines parse left-branching, right-branching, and center-embedding sentences differently.

Formally, a Context-Free Grammar (CFG) is a 4-tuple (N, Σ, I, R) where N is a set of non-terminal symbols, Σ is a finite set of terminal symbols disjoint from N, I is the set of initial nonterminal symbols, and R is a finite set of rules. If it is possible to start from the string α ∈ {N ∪ Σ}* and get the string β ∈ (V ∪ Σ}* by replacing a symbol in α using one of the grammar’s rules, we say α ⇒<sub>G</sub> β, or “β is derived from α using the grammar G.” α ⇒<sub>G</sub>* β means that there exists a chain of derivations to obtain β from α. A CFG G is said to generate a string α if S ⇒<sub>G</sub>* α, where S is the start symbol of the grammar. The language LG = {x: G generates x} is the set of all strings that the grammar can generate.

Each CFG parsing method uses a sequence of *configurations*, which consist of a stack of nonterminal symbols and a string of terminal symbols. Each parsing method involves starting at the starting configuration and applying the parsing steps of the method sequentially until the goal configuration is reached.

The bottom-up parsing method consists of two steps, SHIFT and REDUCE. A SHIFT step causes a terminal symbol x<sub>i</sub> in the input string to be shifted off of the input and onto the stack in the form of a corresponding nonterminal symbol A, where there is a terminal rule A -> x<sub>i</sub> in the grammar’s rule set. A REDUCE step causes one or more nonterminal symbols B<sub>1</sub>, … B<sub>m</sub> at the top of the stack to be replaced by one corresponding nonterminal symbol A, where there is a nonterminal rule A -> B<sub>1</sub>, … B<sub>m</sub> in the grammar’s rule set. The starting configuration for bottom-up parsing is (ε, x<sub>1</sub>…x<sub>n</sub>), meaning that initially the stack has no nonterminal symbols and the input string consists of the entire input. The goal configuration is (A, ε), where A is the start symbol of the grammar. 

Top-down parsing also consists of two steps, MATCH and PREDICT. A MATCH step causes a terminal symbol x<sub>i</sub> in the input string to be matched to a nonterminal symbol A at the top of the stack, where there is a terminal rule rule A -> x<sub>i</sub> in the grammar’s rule set. A PREDICT step causes one nonterminal symbol A at the top of the stack to be replaced with one or more nonterminal symbols B<sub>1</sub>, … B<sub>m</sub>, where there is a nonterminal rule A -> B<sub>1</sub>, … B<sub>m</sub> in the grammar’s rule set. The starting configuration for bottom-up parsing is (A, x<sub>1</sub>…x<sub>n</sub>), where A is the start symbol of the grammar and the input string consists of the entire input. The goal configuration is (ε, ε), meaning that both the stack of nonterminals and the input string of terminals have been exhausted.

Unlike the previous two methods, left-corner parsing consists of four steps. Left-corner parsing involves two types of symbols, non-barred symbols corresponding to the bottom-up version of the symbol and barred-symbols (denoted as A’) corresponding to the top-down version of the symbol. The starting configuration is (A’, x<sub>1</sub>…x<sub>n</sub>), where A is the start symbol and the input string consists of the entire input. In the SHIFT step,  a terminal symbol x<sub>1</sub>…x<sub>n</sub> in the input string is shifted off of the input and onto the stack in the form of a corresponding nonterminal symbol A, where there is a terminal rule A -> x<sub>i</sub> in the grammar’s rule set. In the MATCH step, a terminal symbol x<sub>i</sub> in the input string is matched to a nonterminal symbol A' at the top of the stack, where there is a terminal rule rule A -> x<sub>i</sub> in the grammar’s rule set. In the PREDICT step, an unbarred nonterminal symbol B<sub>1</sub> at the top of the stack is replaced by a sequence of one or more barred nonterminal symbols B<sub>2</sub>’, …, B<sub>m</sub>’ and an unbarred nonterminal symbol A, where there is a rule A -> B<sub>1</sub>, … B<sub>m</sub> in the grammar’s rule set. In the CONNECT step, an unbarred nonterminal symbol B<sub>1</sub> and a barred nonterminal symbol A at the top of the stack are replaced by a sequence of one or more barred nonterminal symbols B<sub>2</sub>', …, B<sub>m</sub>', where there is a nonterminal rule A -> B<sub>1</sub>, … B<sub>m</sub> in the grammar’s rule set. The goal configuration is (ε, ε), meaning that both the stack of nonterminals and the input string of terminals have been exhausted.

I implemented these parsing methods by translating these formalisms into Haskell code. For explanations of my implementation, see the comments on the code file MainProject01.hs.

# Connection to Human Language Faculties

The motivation behind analyzing these parsing methods is to compare and contrast the parsing of left-branching, right-branching, and center-embedding sentence structures.

Examples of left-branching structures include:\
a. Mary won\
b. Mary’s baby won\
c. Mary’s boss’s baby won

Examples of right-branching structures include:\
a. John met the boy\
b. John met the boy that saw the actor\
c. John met the boy that saw the actor that won the award

Examples of center-embedding structures include:\
a. the actor won\
b. the actor the boy met won\
c. the actor the boy the baby saw met won

By investigating how the size of the stack for each parsing method varies with the sentence structure (whether it is left-branching, right-branching, or center-embedding), we can gain insight into how humans process language. It can be thought that humans have a mental stack in their brain analogous to the stack used in these parsing algorithms. Depending on the parsing method being used, the length of the stack grows for some types of structures and not for others. Linguistic studies have shown that humans can process left-branching structures (of the form “Mary’s boss’s baby saw”) and right-branching structures (of the form “John met the boy that saw the actor”) with relative ease but struggle to process longer center-embedding structures such as “the actor the boy the baby saw met won.” Analyses of the various parsing algorithms show that with bottom-up parsing, stack length remains fixed for left-branching structures but increases for right-branching and center-embedding structures. With top-down parsing, stack length remains fixed for right-branching structures but increases for the other types. With left-corner parsing, stack length remains fixed for left- and right-branching structures but increases for center-embedding structures. Thus, left-corner parsing seems to most closely resemble the processing capabilities of the human brain.

# Analysis of Code
While my bottom-up parser did not appear to struggle with any inputs, initially, my top-down parser was unable to handle instances where there was left recursion in the grammar, namely when the grammar has rules of the form A -> AB<sub>1</sub>…B<sub>n</sub>. Such rules would cause the function to go into an infinite loop. An example of sentences that triggered this issue were left-branching structures such as “Mary’s baby won” or “Mary’s boss’s baby won.” This issue was not present with bottom-up parsing because unlike top-down parsing which uses PREDICT to manipulate nonterminal symbols in the stack, bottom-up parsing uses REDUCE, which, as its name suggests, only reduces the length of the stack. However, the opposite occurs for top-down parsing, as instead of reducing the size of the stack, PREDICT expands it. This means that a rule with left-recursion could be applied infinitely if the function only considers the top of the stack. 

Fixing this issue requires comparing the length of the stack to the length of the input. The key observation for knowing when to terminate the recursion is that with top-down parsing, each nonterminal symbol that is PREDICTed onto the stack must eventually be MATCHed to a terminal symbol from the input string. Once the length of the stack exceeds the length of the input string, there will exist a nonterminal symbol on the stack that will not be matchable to any element of the input, implying that the goal configuration will never be reached. Thus, we can add a check to ensure that the length of the stack is less than or equal to that of the input string before returning a new ParseStep in the PREDICT method. Since each PREDICT step increases the length of the stack without changing the length of the input string, adding this extra condition would ensure that each recursive step of the algorithm brings it closer to termination. Once I implemented this check, the issue of the infinite loop was resolved.

However, my implementation of left-corner parsing also seemed to go into an infinite loop for some sentences, namely right-branching sentences like “John met the boy that saw the actor that won the award” and longer center-embedding sentences like “the actor the boy the baby saw met won.” It is possible for the algorithm to go in an infinite loop of PREDICT and SHIFT, where after each PREDICT step, a SHIFT step shifts an unbarred symbol onto the stack which allows PREDICT to apply again, and so on. The same check from earlier that the length of the stack must not exceed the length of the input string cannot be applied for left-corner parsing. With top-down parsing, the only way to shorten the length of the stack to bring it closer to the goal configuration is to MATCH nonterminal symbols on the stack to terminal symbols in the input. Whereas with left-corner parsing, there are other ways aside from MATCH to shorten the stack length, namely LC-CONNECT. Thus, the correct check is that the number of barred symbols in the stack must not exceed the length of the input string, as only barred symbols are eligible for the stack-shortening operations of MATCH and LC-CONNECT. Once this modified check was implemented, the infinite loop issue resolved. 

Counterintuitively, the runtime of the algorithms did not seem to correlate whatsoever with the size of the stack. Although one would expect left-corner parsing to have the lowest runtime on average since the length of the stack does not increase for left- and right-branching structures, it actually has the largest runtime. Upon gathering data and running tests, I discovered that the sentences with the largest runtime are the ones with the shortest stack depth. 

Analyzing the runtime of the parsing algorithms explains this counterintuitive phenomenon. The algorithm for the parser recursively explores all possible paths in a tree-like fashion. This results in an exponential runtime of at least O(2<sup>n</sup>) for top-down and bottom-up parsing, as the recursive tree branches out for each of the two types of transition functions (SHIFT and REDUCE for bottom-up and MATCH and PREDICT for top-down). In contrast, as left-corner parsing has the greatest amount of transition functions (four as opposed to two), the exponential runtime becomes dominated by O(4<sup>n</sup>). 

# Areas for Further Research
An area in which I would like to extend this investigation is to reduce the runtime of the parsing algorithms. One key strategy would be dynamic programming, which would involve memoizing the paths of the recursive tree so that a path does not have to be visited more than once. This could reduce the runtime from exponential to polynomial. Another strategy would be more careful backtracking based on paths that are not likely to result in sensible parses. This could include implementing probabilistic or weighted CFGs to penalize paths that are unlikely to result in sensible parses. Finally, simple tweaks such as hashing the rules of the CFG in a hash table could mitigate the O(n) traversal of the rule list in each of the transition functions.

In addition, as part of testing my code to ensure robustness, I created a CFG meant to generate sentences in Italian. All three parsing algorithms were able to parse the Italian sentences. For sentences where multiple parses were possible based on the provided rules, all were returned. Not all parses made sense, however. For example, the determiner “la” can both be a definite article and an object pronoun, hence the rules NP -> “la” and D -> “la.” The parsing algorithms would return both possibilities even if only one made sense in the context of the sentence. This highlights a fundamental issue with CFGs – as their name suggests, they cannot consider the context behind strings. This is also why they cannot generate languages of the form L={ ωω: ω ∈{0,1}*}, which requires context about the first word of the string in order to be able to duplicate the word exactly. Thus, this brings up a possible area for further research or extending the model. It would be interesting to investigate how or even if these parsing algorithms can be modified to only output “sensible” parses. For example, it might be possible to attach weights or probabilities to penalize certain paths that are less likely to lead to sensible parses. It would also be interesting to investigate how mildly context-sensitive grammars could be used to accomplish such a task. 

Yet, the fundamental workings of the algorithms did not seem to be challenged just by a change of language from English to Italian. This brings up a separate area for further research which would be to investigate whether speakers of languages with syntactical rules that differ significantly from those of English still have the same limitations with the stack capacities of their brains. 
